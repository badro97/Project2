{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "metadata": {
        "id": "Hqy65jY-6jU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNpHHc845xAV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from category_encoders import OrdinalEncoder\n",
        "from category_encoders import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from sklearn.compose import make_column_selector\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from yellowbrick.model_selection import feature_importances\n",
        "from yellowbrick.features import pca_decomposition\n",
        "from yellowbrick.target import class_balance\n",
        "from yellowbrick.target.feature_correlation import feature_correlation\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Measurement_summary.csv')\n",
        "df.info()"
      ],
      "metadata": {
        "id": "oMHIZ16q6mF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "ZJKUXb1JEg8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "M-m7-fZlFh4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "62Q4b4LKFlea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "CSwlHfK0GJZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "bW49XOGoF36I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tt = df.copy()\n",
        "df_tt['Measurement date'] = df_tt['Measurement date'].astype('datetime64')\n",
        "df_tt['hour'] = df_tt.loc[:, \"Measurement date\"].dt.hour\n",
        "df_tt = df_tt.drop('Measurement date', axis=1)\n",
        "\n",
        "time = df_tt.groupby('hour', as_index=False).agg({'PM10':'mean', 'PM2.5':'mean'})\n",
        "time.plot(x='hour', y=['PM10', 'PM2.5'])"
      ],
      "metadata": {
        "id": "zsrloAn_grg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y = df.copy()\n",
        "df_y['Measurement date'] = df_y['Measurement date'].astype('datetime64')\n",
        "df_y['Year'] = df_y.loc[:, \"Measurement date\"].dt.year\n",
        "df_y = df_y.drop('Measurement date', axis=1)\n",
        "\n",
        "year = df_y.groupby('Year', as_index=False).agg({'SO2':'mean', 'NO2':'mean', 'O3':'mean', 'CO':'mean', 'PM10':'mean', 'PM2.5':'mean'})\n",
        "plt.plot(year.Year, year['PM10'], label='PM10')\n",
        "plt.plot(year.Year, year['PM2.5'], label='PM2.5')\n",
        "plt.xticks([2017,2018,2019])\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Ooyc1Uth2Od-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_a = df.copy()\n",
        "df_a = df_a.groupby(['Measurement date'], as_index=False).agg({'SO2':'mean', 'NO2':'mean', 'O3':'mean', 'CO':'mean', 'PM10':'mean', 'PM2.5':'mean'})\n",
        "df_air = df_a.corr()\n",
        "sns.clustermap(df_air, annot=True, cmap='Reds', vmin=-1, vmax=1)"
      ],
      "metadata": {
        "id": "L_2mYKGo3w2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 서울 미세먼지(PM10) 농도\n",
        "location = df.groupby('Station code')['PM10'].agg([np.mean])\n",
        "location['Latitude'] = df['Latitude'].unique()\n",
        "location['Longitude'] = df['Longitude'].unique()\n",
        "\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "# PM10\n",
        "def PM10(x):\n",
        "    if x >= 45:\n",
        "        return 'red'\n",
        "    elif x >= 40:\n",
        "        return 'yellow'\n",
        "    else:\n",
        "        return 'cyan'\n",
        "\n",
        "# Map\n",
        "seoul = folium.Map(location=[37.55138077230307, 126.98712254969668], zoom_start=12)\n",
        "\n",
        "# Circle\n",
        "for i in range(len(location)):\n",
        "    # 관측소\n",
        "    folium.Circle(location=[location.iloc[i,1], location.iloc[i,2]], radius = location.iloc[i, 0]*30, color=PM10(location.iloc[i,0]),fill_color='#ffffgg').add_to(seoul)\n",
        "seoul"
      ],
      "metadata": {
        "id": "s3UTxdtb84gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_t = df.groupby(['Measurement date'], as_index=False).agg({'SO2':'mean', 'NO2':'mean', 'O3':'mean', 'CO':'mean', 'PM10':'mean', 'PM2.5':'mean'})\n",
        "df_t"
      ],
      "metadata": {
        "id": "S2ccwhq7gnHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(4, 2, figsize=(20,15))\n",
        "\n",
        "sns.scatterplot(x='NO2', y= 'PM10', data=df_t, ax=ax[0,0], alpha=.5)\n",
        "sns.scatterplot(x='NO2', y= 'PM2.5', data=df_t, ax=ax[0,1], alpha=.5)\n",
        "\n",
        "sns.scatterplot(x='CO', y= 'PM10', data=df_t, ax=ax[1,0], alpha=.5)\n",
        "sns.scatterplot(x='CO', y= 'PM2.5', data=df_t, ax=ax[1,1], alpha=.5)\n",
        "\n",
        "sns.scatterplot(x='SO2', y= 'PM10', data=df_t, ax=ax[2,0], alpha=.5)\n",
        "sns.scatterplot(x='SO2', y= 'PM2.5', data=df_t, ax=ax[2,1], alpha=.5)\n",
        "\n",
        "sns.scatterplot(x='O3', y= 'PM10', data=df_t, ax=ax[3,0], alpha=.5)\n",
        "sns.scatterplot(x='O3', y= 'PM2.5', data=df_t, ax=ax[3,1], alpha=.5)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "iVVoiqFbJRtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x='NO2', y= 'PM10', data=df_t, label='NO2')\n",
        "sns.scatterplot(x='O3', y= 'PM10', data=df_t, label='O3')\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "dzMgdcwJLHP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x='NO2', y= 'PM2.5', data=df_t, label='NO2')\n",
        "sns.scatterplot(x='O3', y= 'PM2.5', data=df_t, label='O3')\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "4JkqM8lpLX_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_time = df['Measurement date'].str.split(\" \",n=1,expand=True)\n",
        "# df['date'] = df_time[0]\n",
        "# df['time'] = df_time[1]\n",
        "# df = df.drop(['Measurement date'],axis = 1)\n",
        "# df_add = df.loc[:,['SO2','NO2','O3','CO','PM10','PM2.5','time']]\n",
        "\n",
        "# plt.figure(figsize = (15,13))\n",
        "# plt.bar(df_add['time'],df_add['PM10'],color = 'aquamarine')\n",
        "# plt.title('Concentration of fine dust by time',fontsize = 20)\n",
        "# plt.xlabel('Time',fontsize=15)\n",
        "# plt.ylabel('Concentration',fontsize = 15)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "J6WagMX6oRh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff = df.groupby(['Measurement date'], as_index=False).agg({'SO2':'mean', 'NO2':'mean', 'O3':'mean', 'CO':'mean', 'PM10':'mean', 'PM2.5':'mean'})"
      ],
      "metadata": {
        "id": "ghJcNhCh971o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PM2.5 Sub-Index calculation\n",
        "def get_PM25_subindex(x):\n",
        "    if x <= 30:\n",
        "        return x * 50 / 30\n",
        "    elif x <= 60:\n",
        "        return 50 + (x - 30) * 50 / 30\n",
        "    elif x <= 90:\n",
        "        return 100 + (x - 60) * 100 / 30\n",
        "    elif x <= 120:\n",
        "        return 200 + (x - 90) * 100 / 30\n",
        "    elif x <= 250:\n",
        "        return 300 + (x - 120) * 100 / 130\n",
        "    elif x > 250:\n",
        "        return 400 + (x - 250) * 100 / 130\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "dff[\"PM2.5_SubIndex\"] = dff[\"PM2.5\"].apply(lambda x: get_PM25_subindex(x))\n",
        "\n",
        "## PM10 Sub-Index calculation\n",
        "def get_PM10_subindex(x):\n",
        "    if x <= 50:\n",
        "        return x\n",
        "    elif x <= 100:\n",
        "        return x\n",
        "    elif x <= 250:\n",
        "        return 100 + (x - 100) * 100 / 150\n",
        "    elif x <= 350:\n",
        "        return 200 + (x - 250)\n",
        "    elif x <= 430:\n",
        "        return 300 + (x - 350) * 100 / 80\n",
        "    elif x > 430:\n",
        "        return 400 + (x - 430) * 100 / 80\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "dff[\"PM10_SubIndex\"] = dff[\"PM10\"].apply(lambda x: get_PM10_subindex(x))\n",
        "\n",
        "## SO2 Sub-Index calculation\n",
        "def get_SO2_subindex(x):\n",
        "    if x <= 40:\n",
        "        return x * 50 / 40\n",
        "    elif x <= 80:\n",
        "        return 50 + (x - 40) * 50 / 40\n",
        "    elif x <= 380:\n",
        "        return 100 + (x - 80) * 100 / 300\n",
        "    elif x <= 800:\n",
        "        return 200 + (x - 380) * 100 / 420\n",
        "    elif x <= 1600:\n",
        "        return 300 + (x - 800) * 100 / 800\n",
        "    elif x > 1600:\n",
        "        return 400 + (x - 1600) * 100 / 800\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "dff[\"SO2_SubIndex\"] = dff[\"SO2\"].apply(lambda x: get_SO2_subindex(x))\n",
        "\n",
        "## NO2 Sub-Index calculation\n",
        "def get_NO2_subindex(x):\n",
        "    if x <= 40:\n",
        "        return x * 50 / 40\n",
        "    elif x <= 80:\n",
        "        return 50 + (x - 40) * 50 / 40\n",
        "    elif x <= 180:\n",
        "        return 100 + (x - 80) * 100 / 100\n",
        "    elif x <= 280:\n",
        "        return 200 + (x - 180) * 100 / 100\n",
        "    elif x <= 400:\n",
        "        return 300 + (x - 280) * 100 / 120\n",
        "    elif x > 400:\n",
        "        return 400 + (x - 400) * 100 / 120\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "dff[\"NO2_SubIndex\"] = dff[\"NO2\"].apply(lambda x: get_NO2_subindex(x))\n",
        "\n",
        "## CO Sub-Index calculation\n",
        "def get_CO_subindex(x):\n",
        "    if x <= 1:\n",
        "        return x * 50 / 1\n",
        "    elif x <= 2:\n",
        "        return 50 + (x - 1) * 50 / 1\n",
        "    elif x <= 10:\n",
        "        return 100 + (x - 2) * 100 / 8\n",
        "    elif x <= 17:\n",
        "        return 200 + (x - 10) * 100 / 7\n",
        "    elif x <= 34:\n",
        "        return 300 + (x - 17) * 100 / 17\n",
        "    elif x > 34:\n",
        "        return 400 + (x - 34) * 100 / 17\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "dff[\"CO_SubIndex\"] = dff[\"CO\"].apply(lambda x: get_CO_subindex(x))\n",
        "\n",
        "## O3 Sub-Index calculation\n",
        "def get_O3_subindex(x):\n",
        "    if x <= 50:\n",
        "        return x * 50 / 50\n",
        "    elif x <= 100:\n",
        "        return 50 + (x - 50) * 50 / 50\n",
        "    elif x <= 168:\n",
        "        return 100 + (x - 100) * 100 / 68\n",
        "    elif x <= 208:\n",
        "        return 200 + (x - 168) * 100 / 40\n",
        "    elif x <= 748:\n",
        "        return 300 + (x - 208) * 100 / 539\n",
        "    elif x > 748:\n",
        "        return 400 + (x - 400) * 100 / 539\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "dff[\"O3_SubIndex\"] = dff[\"O3\"].apply(lambda x: get_O3_subindex(x))\n",
        "\n",
        "## AQI bucketing\n",
        "def get_AQI_bucket(x):\n",
        "    if x <= 50:\n",
        "        return \"Good\"\n",
        "    elif x <= 100:\n",
        "        return \"Satisfactory\"\n",
        "    elif x <= 200:\n",
        "        return \"Moderate\"\n",
        "    elif x <= 300:\n",
        "        return \"Bad\"\n",
        "    elif x <= 400:\n",
        "        return \"Very Bad\"\n",
        "    elif x > 400:\n",
        "        return \"Hell\"\n",
        "    else:\n",
        "        return np.NaN\n",
        "\n",
        "dff[\"AQI_score\"] = round(dff[[\"PM2.5_SubIndex\",\"PM10_SubIndex\",\"SO2_SubIndex\",\"NO2_SubIndex\",\"CO_SubIndex\",\"O3_SubIndex\"]].max(axis = 1))\n",
        "\n",
        "dff[\"AQI_warn\"] = dff[\"AQI_score\"].apply(lambda x: get_AQI_bucket(x))\n",
        "# dff.drop([\"PM2.5_SubIndex\",\"PM10_SubIndex\",\"SO2_SubIndex\",\"NO2_SubIndex\",\"CO_SubIndex\",\"O3_SubIndex\"], axis=1, inplace=True)\n",
        "dff.drop([\"PM2.5\",\"PM10\",\"SO2\",\"NO2\",\"CO\",\"O3\"], axis=1, inplace=True)\n",
        "dff.head(13)"
      ],
      "metadata": {
        "id": "JAzuXoOu96vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.DataFrame(dff.AQI_warn.value_counts())\n",
        "b = pd.DataFrame(dff.AQI_warn.value_counts(normalize=True))\n",
        "a.rename(columns={'AQI_warn':'Counts'},inplace=True)\n",
        "b.rename(columns={'AQI_warn':'proportion'},inplace=True)\n",
        "c = pd.concat([a, b], axis = 1)\n",
        "c"
      ],
      "metadata": {
        "id": "5MRR-IqhxvNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 훈련/검증/테스트 split\n",
        "train, test = train_test_split(dff, test_size=0.2, random_state=2)\n",
        "train, val = train_test_split(train, test_size=len(test), random_state=2)\n",
        "train.shape, val.shape, test.shape"
      ],
      "metadata": {
        "id": "0zxOfijAWo1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'AQI_warn'\n",
        "\n",
        "features = dff.columns.drop([target,\n",
        "                             'Measurement date',\n",
        "                             'AQI_score',\n",
        "                             'PM10_SubIndex',\n",
        "                             'PM2.5_SubIndex'\n",
        "                             ])\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "\n",
        "X_test = test[features]\n",
        "y_test = test[target]"
      ],
      "metadata": {
        "id": "-5J26c0PW3Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_Dtrees(X_train, y_train):\n",
        "    pipe = None    \n",
        "    clf = None     \n",
        "\n",
        "    pipe = make_pipeline(\n",
        "        SimpleImputer(),\n",
        "        StandardScaler(), \n",
        "        DecisionTreeClassifier(random_state=42, criterion='entropy')\n",
        "    )\n",
        "    params = {\n",
        "        \"decisiontreeclassifier__max_depth\" : [3,4,5,6,7],\n",
        "        \"decisiontreeclassifier__min_samples_split\" :[2,3]\n",
        "    }\n",
        "    \n",
        "    clf = RandomizedSearchCV(\n",
        "        pipe, \n",
        "        param_distributions=params, \n",
        "        n_iter=10, \n",
        "        cv=5,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    return clf"
      ],
      "metadata": {
        "id": "hARbOPNUYcWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_D = fit_Dtrees(X_train, y_train)\n",
        "\n",
        "dt = clf_D.best_estimator_\n",
        "\n",
        "pred_train = dt.predict(X_train)\n",
        "pred_val = dt.predict(X_val) \n",
        "pred_test = dt.predict(X_test) \n",
        "\n",
        "print('Train 정확도:', accuracy_score(y_train, pred_train))\n",
        "print('val 정확도:', accuracy_score(y_val, pred_val))\n",
        "print('Test 정확도:', accuracy_score(y_test, pred_test))"
      ],
      "metadata": {
        "id": "ouxN6vLGYkoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dt = dt.named_steps['decisiontreeclassifier']\n",
        "\n",
        "importances = pd.Series(model_dt.feature_importances_, X_val.columns)\n",
        "plt.figure(figsize=(5,5))\n",
        "importances.sort_values().plot.barh();"
      ],
      "metadata": {
        "id": "nKyDtot4bDEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_depth_list = range(1,20)\n",
        "\n",
        "train_acc_list = []\n",
        "val_acc_list = []\n",
        "for max_depth in max_depth_list:\n",
        "    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=2)\n",
        "    tree.fit(X_train, y_train)\n",
        "\n",
        "    pred_train = tree.predict(X_train)\n",
        "    pred_val = tree.predict(X_val)\n",
        "\n",
        "    train_acc_list.append(accuracy_score(y_train, pred_train))\n",
        "    val_acc_list.append(accuracy_score(y_val, pred_val))\n",
        "\n",
        "import pandas as pd\n",
        "d = {\n",
        "    \"max_depth\":max_depth_list,\n",
        "    \"Train_acc\":train_acc_list,\n",
        "    \"Val_acc\":val_acc_list\n",
        "}\n",
        "acc_df = pd.DataFrame(d)\n",
        "acc_df.set_index('max_depth').plot(figsize=(10,7))\n",
        "plt.xticks(max_depth_list)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rKdpCV01ex17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "pcm = plot_confusion_matrix(dt, X_val, y_val,\n",
        "                            cmap=plt.cm.Blues,\n",
        "                            ax=ax)\n",
        "plt.title(f'Confusion matrix, n = {len(y_val)}', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n874IMbzfvYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = dt.predict_proba(X_val)[:, 1]\n",
        "pred_proba = pd.DataFrame({\n",
        "    'y_val': y_val,\n",
        "    'pred_proba': y_pred_proba})\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "label=list(dff.AQI_warn.unique())\n",
        "for i in label:\n",
        "  fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba, pos_label=i)\n",
        "\n",
        "  roc = pd.DataFrame({\n",
        "      'FPR(Fall-out)': fpr, \n",
        "      'TPRate(Recall)': tpr, \n",
        "      'Threshold': thresholds\n",
        "  })\n",
        "  plt.scatter(fpr, tpr, label=i)\n",
        "  plt.legend()\n",
        "  plt.title('ROC curve')\n",
        "  plt.xlabel('FPR(Fall-out)')\n",
        "  plt.ylabel('TPR(Recall)');\n"
      ],
      "metadata": {
        "id": "ePL7ckxEf66Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randf= make_pipeline(\n",
        "    SimpleImputer(),\n",
        "    RandomForestClassifier(random_state=10)\n",
        ")\n",
        "randf.fit(X_train, y_train)\n",
        "pred = randf.predict(X_val)\n",
        "print('RandomForest 정확도(튜닝X):', accuracy_score(y_val, pred))"
      ],
      "metadata": {
        "id": "a8jSAGhmJe1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = RandomForestClassifier(random_state=10)\n",
        "params = { \n",
        "    'n_estimators': [200, 500],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth' : [4,5,6,7,8],\n",
        "    'criterion' :['gini', 'entropy']\n",
        "}\n",
        "grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2 ,verbose=1)\n",
        "grid_cv.fit(X_train , y_train)\n",
        "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
        "print('최고 예측 정확도:', grid_cv.best_score_)\n",
        "\n",
        "rf = grid_cv.best_estimator_\n",
        "pred = rf.predict(X_val)\n",
        "print('RandomForest 정확도(튜닝O)', accuracy_score(y_val, pred))"
      ],
      "metadata": {
        "id": "DllsKqXh65s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "pcm = plot_confusion_matrix(randf, X_test, y_test,\n",
        "                            cmap=plt.cm.Blues,\n",
        "                            ax=ax)\n",
        "plt.title(f'Confusion matrix, n = {len(y_test)}', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bYh4_SZkReZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = make_pipeline(\n",
        "    SimpleImputer(),\n",
        "    GradientBoostingClassifier(random_state=10)\n",
        ")\n",
        "model2.fit(X_train, y_train)\n",
        "pred = model2.predict(X_val)\n",
        "print('GBM 정확도(튜닝X):', accuracy_score(y_val, pred))"
      ],
      "metadata": {
        "id": "1Poi44teZ9Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_clf = GradientBoostingClassifier(random_state=10)\n",
        "params = {\n",
        "        \"learning_rate\" : [0.05,0.1],\n",
        "        \"n_estimators\" :[1000,1500]\n",
        "    }\n",
        "grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1)\n",
        "grid_cv.fit(X_train , y_train)\n",
        "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
        "print('최고 예측 정확도:', grid_cv.best_score_)\n",
        "\n",
        "gbm = grid_cv.best_estimator_\n",
        "pred = gbm.predict(X_val)\n",
        "print('GBM 정확도(튜닝O)', accuracy_score(y_val, pred))"
      ],
      "metadata": {
        "id": "SoLTL3wsLs_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "pcm = plot_confusion_matrix(gbm, X_val, y_val,\n",
        "                            cmap=plt.cm.Blues,\n",
        "                            ax=ax)\n",
        "plt.title(f'Confusion matrix, n = {len(y_val)}', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JrHppEzARjjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgbc = make_pipeline(\n",
        "    SimpleImputer(),\n",
        "    XGBClassifier(random_state=10)\n",
        ")\n",
        "xgbc.fit(X_train, y_train)\n",
        "pred = xgbc.predict(X_val)\n",
        "print('XGB 정확도(튜닝X):', accuracy_score(y_val, pred))"
      ],
      "metadata": {
        "id": "RNSOkJEa6eGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_clf = XGBClassifier(random_state=10)\n",
        "params = {\n",
        "        \"learning_rate\" : [0.05,0.1],\n",
        "        \"n_estimators\" :[1000,1500]\n",
        "    }\n",
        "grid_cv = GridSearchCV(xgb_clf , param_grid=params , cv=2 ,verbose=1)\n",
        "grid_cv.fit(X_train , y_train)\n",
        "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
        "print('최고 예측 정확도:', grid_cv.best_score_)\n",
        "\n",
        "xgb = grid_cv.best_estimator_\n",
        "pred = xgb.predict(X_val)\n",
        "print('XGB 정확도(튜닝O)', accuracy_score(y_val, pred))"
      ],
      "metadata": {
        "id": "A-wiSQujEyWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "pcm = plot_confusion_matrix(xgbc, X_val, y_val,\n",
        "                            cmap=plt.cm.Blues,\n",
        "                            ax=ax)\n",
        "plt.title(f'Confusion matrix, n = {len(y_val)}', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z-CTDmPMJHlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model):\n",
        "  \n",
        "  pred = model.predict(X_test)\n",
        "  \n",
        "  print('정확도 :', accuracy_score(y_test, pred))\n",
        "  print('정밀도 :', precision_score(y_test, pred, average='weighted'))\n",
        "  print('재현율 :', recall_score(y_test, pred, average='weighted'))\n",
        "  print('f1 score :', f1_score(y_test, pred, average='weighted'))\n"
      ],
      "metadata": {
        "id": "6CSpgtin8Zyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(dt)"
      ],
      "metadata": {
        "id": "R4E6w6ofFmfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(randf)"
      ],
      "metadata": {
        "id": "U4iwA8wgFXOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(gbm)"
      ],
      "metadata": {
        "id": "0Pi5rfvHD7yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(xgbc)"
      ],
      "metadata": {
        "id": "RCLZcPGnFw_e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}